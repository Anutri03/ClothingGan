{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm8iDDKC1LZo"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Anutri03/ClothingGAN/blob/master/ClothingGAN_Demo.ipynb)\n",
        "# Clothing GAN demo\n",
        "\n",
        "\n",
        "<br>\n",
        "Make sure runtime type is GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Kj8mGkmH0xgA",
        "outputId": "6a793110-884d-4f59-89ec-9c5eced9b98a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 200})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Install dependencies (restart runtime after installing)\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
        "#!pip install ninja gradio fbpca boto3 requests==2.23.0 urllib3==1.25.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwf-gggHtA_t",
        "outputId": "68f7afee-6889-4f9c-97d0-de9ba931c206"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Anutri03/ClothingGAN.git\n",
        "#%cd ClothingGAN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "2BYsIETVtGnF",
        "outputId": "e3b5c2ac-9c18-4917-de0f-1f2c126aa696"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 200})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Anurag\n",
            "[nltk_data]     Tripathi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Install other dependencies\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
        "!git submodule update --init --recursive\n",
        "!python -c \"import nltk; nltk.download('wordnet')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJg91yvSwKi3",
        "outputId": "15c2285c-e8da-45b9-bdc2-9bab3709b270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StyleGAN2: Optimized CUDA op FusedLeakyReLU not available, using native PyTorch fallback.\n",
            "StyleGAN2: Optimized CUDA op UpFirDn2d not available, using native PyTorch fallback.\n"
          ]
        }
      ],
      "source": [
        "#@title Load Model\n",
        "selected_model = 'lookbook'\n",
        "\n",
        "# Load model\n",
        "from IPython.utils import io\n",
        "import torch\n",
        "import PIL\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from models import get_instrumented_model\n",
        "from decomposition import get_or_compute\n",
        "from config import Config\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# Speed up computation\n",
        "torch.autograd.set_grad_enabled(False)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Specify model to use\n",
        "config = Config(\n",
        "  model='StyleGAN2',\n",
        "  layer='style',\n",
        "  output_class=selected_model,\n",
        "  components=80,\n",
        "  use_w=True,\n",
        "  batch_size=5_000, # style layer quite small\n",
        ")\n",
        "\n",
        "inst = get_instrumented_model(config.model, config.output_class,\n",
        "                              config.layer, torch.device('cuda'), use_w=config.use_w)\n",
        "\n",
        "path_to_components = get_or_compute(config, inst)\n",
        "\n",
        "model = inst.model\n",
        "\n",
        "comps = np.load(path_to_components)\n",
        "lst = comps.files\n",
        "latent_dirs = []\n",
        "latent_stdevs = []\n",
        "\n",
        "load_activations = False\n",
        "\n",
        "for item in lst:\n",
        "    if load_activations:\n",
        "      if item == 'act_comp':\n",
        "        for i in range(comps[item].shape[0]):\n",
        "          latent_dirs.append(comps[item][i])\n",
        "      if item == 'act_stdev':\n",
        "        for i in range(comps[item].shape[0]):\n",
        "          latent_stdevs.append(comps[item][i])\n",
        "    else:\n",
        "      if item == 'lat_comp':\n",
        "        for i in range(comps[item].shape[0]):\n",
        "          latent_dirs.append(comps[item][i])\n",
        "      if item == 'lat_stdev':\n",
        "        for i in range(comps[item].shape[0]):\n",
        "          latent_stdevs.append(comps[item][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "uCR_3Ghos2kK"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "from ipywidgets import fixed\n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n",
        "\n",
        "def name_direction(sender):\n",
        "  if not text.value:\n",
        "    print('Please name the direction before saving')\n",
        "    return\n",
        "    \n",
        "  if num in named_directions.values():\n",
        "    target_key = list(named_directions.keys())[list(named_directions.values()).index(num)]\n",
        "    print(f'Direction already named: {target_key}')\n",
        "    print(f'Overwriting... ')\n",
        "    del(named_directions[target_key])\n",
        "  named_directions[text.value] = [num, start_layer.value, end_layer.value]\n",
        "  save_direction(random_dir, text.value)\n",
        "  for item in named_directions:\n",
        "    print(item, named_directions[item])\n",
        "\n",
        "def save_direction(direction, filename):\n",
        "  filename += \".npy\"\n",
        "  np.save(filename, direction, allow_pickle=True, fix_imports=True)\n",
        "  print(f'Latent direction saved as {filename}')\n",
        "\n",
        "def mix_w(w1, w2, content, style):\n",
        "    for i in range(0,5):\n",
        "        w2[i] = w1[i] * (1 - content) + w2[i] * content\n",
        "\n",
        "    for i in range(5, 16):\n",
        "        w2[i] = w1[i] * (1 - style) + w2[i] * style\n",
        "    \n",
        "    return w2\n",
        "\n",
        "def display_sample_pytorch(seed, truncation, directions, distances, scale, start, end, w=None, disp=True, save=None, noise_spec=None):\n",
        "    # blockPrint()\n",
        "    model.truncation = truncation\n",
        "    if w is None:\n",
        "        w = model.sample_latent(1, seed=seed).detach().cpu().numpy()\n",
        "        w = [w]*model.get_max_latents() # one per layer\n",
        "    else:\n",
        "        w = [np.expand_dims(x, 0) for x in w]\n",
        "    \n",
        "    for l in range(start, end):\n",
        "      for i in range(len(directions)):\n",
        "        w[l] = w[l] + directions[i] * distances[i] * scale\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    #save image and display\n",
        "    out = model.sample_np(w)\n",
        "    final_im = Image.fromarray((out * 255).astype(np.uint8)).resize((500,500),Image.LANCZOS)\n",
        "    \n",
        "    \n",
        "    if save is not None:\n",
        "      if disp == False:\n",
        "        print(save)\n",
        "      final_im.save(f'out/{seed}_{save:05}.png')\n",
        "    if disp:\n",
        "      display(final_im)\n",
        "    \n",
        "    return final_im\n",
        "\n",
        "def generate_mov(seed, truncation, direction_vec, scale, layers, n_frames, out_name = 'out', noise_spec = None, loop=True):\n",
        "  \"\"\"Generates a mov moving back and forth along the chosen direction vector\"\"\"\n",
        "  # Example of reading a generated set of images, and storing as MP4.\n",
        "  %mkdir out\n",
        "  movieName = f'out/{out_name}.mp4'\n",
        "  offset = -10\n",
        "  step = 20 / n_frames\n",
        "  imgs = []\n",
        "  for i in log_progress(range(n_frames), name = \"Generating frames\"):\n",
        "    print(f'\\r{i} / {n_frames}', end='')\n",
        "    w = model.sample_latent(1, seed=seed).cpu().numpy()\n",
        "\n",
        "    model.truncation = truncation\n",
        "    w = [w]*model.get_max_latents() # one per layer\n",
        "    for l in layers:\n",
        "      if l <= model.get_max_latents():\n",
        "          w[l] = w[l] + direction_vec * offset * scale\n",
        "\n",
        "    #save image and display\n",
        "    out = model.sample_np(w)\n",
        "    final_im = Image.fromarray((out * 255).astype(np.uint8))\n",
        "    imgs.append(out)\n",
        "    #increase offset\n",
        "    offset += step\n",
        "  if loop:\n",
        "    imgs += imgs[::-1]\n",
        "  with imageio.get_writer(movieName, mode='I') as writer:\n",
        "    for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
        "        writer.append_data(img_as_ubyte(image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "jneXxZnNwHo5",
        "outputId": "c8e2b76e-3a00-47f5-ba2d-51606f09ee93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Assume model, latent_dirs, mix_w, and display_sample_pytorch are already loaded/imported\n",
        "\n",
        "def generate_image(seed1, seed2, content, style, truncation, \n",
        "                   c0, c1, c2, c3, c4, c5, c6, \n",
        "                   start_layer, end_layer, red, green, blue):\n",
        "    seed1 = int(seed1)\n",
        "    seed2 = int(seed2)\n",
        "\n",
        "    scale = 1\n",
        "    params = {'c0': c0,\n",
        "              'c1': c1,\n",
        "              'c2': c2,\n",
        "              'c3': c3,\n",
        "              'c4': c4,\n",
        "              'c5': c5,\n",
        "              'c6': c6}\n",
        "\n",
        "    param_indexes = {'c0': 0,\n",
        "                     'c1': 1,\n",
        "                     'c2': 2,\n",
        "                     'c3': 3,\n",
        "                     'c4': 4,\n",
        "                     'c5': 5,\n",
        "                     'c6': 6}\n",
        "\n",
        "    directions = []\n",
        "    distances = []\n",
        "    for k, v in params.items():\n",
        "        directions.append(latent_dirs[param_indexes[k]])\n",
        "        distances.append(v)\n",
        "\n",
        "    w1 = model.sample_latent(1, seed=seed1).detach().cpu().numpy()\n",
        "    w1 = [w1] * model.get_max_latents()\n",
        "    im1 = model.sample_np(w1)\n",
        "\n",
        "    w2 = model.sample_latent(1, seed=seed2).detach().cpu().numpy()\n",
        "    w2 = [w2] * model.get_max_latents()\n",
        "    im2 = model.sample_np(w2)\n",
        "    combined_im = np.concatenate([im1, im2], axis=1)\n",
        "    input_im = Image.fromarray((combined_im * 255).astype(np.uint8))\n",
        "\n",
        "    mixed_w = mix_w(w1, w2, content, style)\n",
        "    output_im = display_sample_pytorch(seed1, truncation, directions, distances, scale, int(start_layer), int(end_layer), w=mixed_w, disp=False)\n",
        "\n",
        "    # Apply color transformation\n",
        "    output_array = np.array(output_im).astype(float) / 255.0\n",
        "\n",
        "    # Create a mask to apply color change only to specific areas\n",
        "    mask = (np.mean(output_array, axis=2) > 0.1) & (np.mean(output_array, axis=2) < 0.9)\n",
        "\n",
        "    for c in range(3):\n",
        "        if c == 0:\n",
        "            output_array[:, :, c] = np.where(mask, output_array[:, :, c] * (1 + red / 100), output_array[:, :, c])\n",
        "        elif c == 1:\n",
        "            output_array[:, :, c] = np.where(mask, output_array[:, :, c] * (1 + green / 100), output_array[:, :, c])\n",
        "        else:\n",
        "            output_array[:, :, c] = np.where(mask, output_array[:, :, c] * (1 + blue / 100), output_array[:, :, c])\n",
        "\n",
        "    output_array = np.clip(output_array, 0, 1)\n",
        "    output_array = (output_array * 255).astype(np.uint8)\n",
        "    output_im = Image.fromarray(output_array)\n",
        "\n",
        "    return input_im, output_im\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ClothingGAN\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            seed1 = gr.Number(label=\"Seed 1\", value=1)\n",
        "            seed2 = gr.Number(label=\"Seed 2\", value=2)\n",
        "            content = gr.Slider(0, 1, value=0.5, label=\"Content Mixing\")\n",
        "            style = gr.Slider(0, 1, value=0.5, label=\"Style Mixing\")\n",
        "            truncation = gr.Slider(0, 1, value=0.7, label=\"Truncation\")\n",
        "            start_layer = gr.Number(label=\"Start Layer\", value=0)\n",
        "            end_layer = gr.Number(label=\"End Layer\", value=14)\n",
        "\n",
        "            gr.Markdown(\"### Style directions\")\n",
        "            slider_min_val = -20\n",
        "            slider_max_val = 20\n",
        "\n",
        "            c0 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Sleeve & Size\")\n",
        "            c1 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Dress - Jacket\")\n",
        "            c2 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Female Coat\")\n",
        "            c3 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Coat\")\n",
        "            c4 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Graphics\")\n",
        "            c5 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Dark\")\n",
        "            c6 = gr.Slider(slider_min_val, slider_max_val, step=1, value=0, label=\"Less Cleavage\")\n",
        "\n",
        "            gr.Markdown(\"### Color Adjustment\")\n",
        "            red = gr.Slider(-100, 100, step=1, value=0, label=\"Red % Change\")\n",
        "            green = gr.Slider(-100, 100, step=1, value=0, label=\"Green % Change\")\n",
        "            blue = gr.Slider(-100, 100, step=1, value=0, label=\"Blue % Change\")\n",
        "\n",
        "        with gr.Column():\n",
        "            input_image = gr.Image(label=\"Input Mixed Image\")\n",
        "            output_image = gr.Image(label=\"Output Styled Image\")\n",
        "\n",
        "    inputs = [seed1, seed2, content, style, truncation,\n",
        "              c0, c1, c2, c3, c4, c5, c6,\n",
        "              start_layer, end_layer, red, green, blue]\n",
        "\n",
        "    # Instead of a button, trigger on change of any input\n",
        "    for inp in inputs:\n",
        "        inp.change(fn=generate_image, inputs=inputs, outputs=[input_image, output_image])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.8\n",
            "True\n",
            "NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "print(drv.Device(0).name())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "print(cuda.Device(0).name())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Anutri03/ClothingGAN/blob/master/ClothingGAN_Demo.ipynb)\n",
        "# Clothing GAN demo\n",
        "\n",
        "\n",
        "<br>\n",
        "Make sure runtime type is GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11, 8, 0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pycuda.driver as drv\n",
        "drv.get_version()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ClothingGAN-Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
